{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "<|im_start|>system\n",
      "You are Qwen, a helpful AI assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "give me a short instruction to large language model<|im_end|>\n",
      "<|im_start|>assistant\n",
      "\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,    264,\n",
      "          10950,  15235,  17847, 151645,    198, 151644,    872,    198,  46430,\n",
      "            752,    264,   2805,   7600,    311,   3460,   4128,   1614, 151645,\n",
      "            198, 151644,  77091,    198]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='mps:0')}\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mRuntimeError\u001b[39m                              Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[9]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     52\u001b[39m \u001b[38;5;28mprint\u001b[39m(model_inputs)\n\u001b[32m     54\u001b[39m \u001b[38;5;66;03m# Generate response\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m generated_ids = \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mgenerate\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m     56\u001b[39m \u001b[43m    \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mmodel_inputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m     57\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmax_new_tokens\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m512\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m     58\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdo_sample\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Enable sampling for more diverse output\u001b[39;49;00m\n\u001b[32m     59\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtemperature\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.7\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Moderate randomness\u001b[39;49;00m\n\u001b[32m     60\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtop_p\u001b[49m\u001b[43m=\u001b[49m\u001b[32;43m0.9\u001b[39;49m\u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# Nucleus sampling\u001b[39;49;00m\n\u001b[32m     61\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m     63\u001b[39m \u001b[38;5;66;03m# Decode only the newly generated tokens\u001b[39;00m\n\u001b[32m     64\u001b[39m generated_ids = [\n\u001b[32m     65\u001b[39m     output_ids[\u001b[38;5;28mlen\u001b[39m(input_ids):] \u001b[38;5;28;01mfor\u001b[39;00m input_ids, output_ids \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(model_inputs.input_ids, generated_ids)\n\u001b[32m     66\u001b[39m ]\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/works/startup/hands-on-llm/venv/lib/python3.11/site-packages/torch/utils/_contextlib.py:116\u001b[39m, in \u001b[36mcontext_decorator.<locals>.decorate_context\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    113\u001b[39m \u001b[38;5;129m@functools\u001b[39m.wraps(func)\n\u001b[32m    114\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mdecorate_context\u001b[39m(*args, **kwargs):\n\u001b[32m    115\u001b[39m     \u001b[38;5;28;01mwith\u001b[39;00m ctx_factory():\n\u001b[32m--> \u001b[39m\u001b[32m116\u001b[39m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/works/startup/hands-on-llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:2004\u001b[39m, in \u001b[36mGenerationMixin.generate\u001b[39m\u001b[34m(self, inputs, generation_config, logits_processor, stopping_criteria, prefix_allowed_tokens_fn, synced_gpus, assistant_model, streamer, negative_prompt_ids, negative_prompt_attention_mask, **kwargs)\u001b[39m\n\u001b[32m   2001\u001b[39m batch_size = inputs_tensor.shape[\u001b[32m0\u001b[39m]\n\u001b[32m   2003\u001b[39m device = inputs_tensor.device\n\u001b[32m-> \u001b[39m\u001b[32m2004\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_prepare_special_tokens\u001b[49m\u001b[43m(\u001b[49m\u001b[43mgeneration_config\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs_has_attention_mask\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2006\u001b[39m \u001b[38;5;66;03m# decoder-only models must use left-padding for batched generation.\u001b[39;00m\n\u001b[32m   2007\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m.config.is_encoder_decoder \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():\n\u001b[32m   2008\u001b[39m     \u001b[38;5;66;03m# If `input_ids` was given, check if the last id in any sequence is `pad_token_id`\u001b[39;00m\n\u001b[32m   2009\u001b[39m     \u001b[38;5;66;03m# Note: If using, `inputs_embeds` this check does not work, because we want to be more hands-off.\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/works/startup/hands-on-llm/venv/lib/python3.11/site-packages/transformers/generation/utils.py:1853\u001b[39m, in \u001b[36mGenerationMixin._prepare_special_tokens\u001b[39m\u001b[34m(self, generation_config, kwargs_has_attention_mask, device)\u001b[39m\n\u001b[32m   1847\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1848\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m`decoder_start_token_id` or `bos_token_id` has to be defined for encoder-decoder generation.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1849\u001b[39m     )\n\u001b[32m   1850\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torchdynamo_compiling():  \u001b[38;5;66;03m# Checks that depend on tensor-dependent control flow\u001b[39;00m\n\u001b[32m   1851\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[32m   1852\u001b[39m         eos_token_tensor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m1853\u001b[39m         \u001b[38;5;129;01mand\u001b[39;00m \u001b[43misin_mps_friendly\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[43m=\u001b[49m\u001b[43meos_token_tensor\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m=\u001b[49m\u001b[43mpad_token_tensor\u001b[49m\u001b[43m)\u001b[49m.any()\n\u001b[32m   1854\u001b[39m     ):\n\u001b[32m   1855\u001b[39m         \u001b[38;5;28;01mif\u001b[39;00m kwargs_has_attention_mask \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m kwargs_has_attention_mask:\n\u001b[32m   1856\u001b[39m             logger.warning_once(\n\u001b[32m   1857\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33mThe attention mask is not set and cannot be inferred from input because pad token is same as \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1858\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33meos token. As a consequence, you may observe unexpected behavior. Please pass your input\u001b[39m\u001b[33m'\u001b[39m\u001b[33ms \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1859\u001b[39m                 \u001b[33m\"\u001b[39m\u001b[33m`attention_mask` to obtain reliable results.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1860\u001b[39m             )\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~/works/startup/hands-on-llm/venv/lib/python3.11/site-packages/transformers/pytorch_utils.py:344\u001b[39m, in \u001b[36misin_mps_friendly\u001b[39m\u001b[34m(elements, test_elements)\u001b[39m\n\u001b[32m    341\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m elements.tile(test_elements.shape[\u001b[32m0\u001b[39m], \u001b[32m1\u001b[39m).eq(test_elements.unsqueeze(\u001b[32m1\u001b[39m)).sum(dim=\u001b[32m0\u001b[39m).bool().squeeze()\n\u001b[32m    342\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    343\u001b[39m     \u001b[38;5;66;03m# Note: don't use named arguments in `torch.isin`, see https://github.com/pytorch/pytorch/issues/126045\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m344\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43misin\u001b[49m\u001b[43m(\u001b[49m\u001b[43melements\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtest_elements\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[31mRuntimeError\u001b[39m: isin_Tensor_Tensor_out only works on floating types on MPS for pre MacOS_14_0. Received dtype: Long"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "\n",
    "\"\"\"\n",
    "pip install ipywidgets\n",
    "\n",
    "then restart jupyter, will show progress bar\n",
    "\"\"\"\n",
    "\n",
    "model_name='Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "# for mac m1\n",
    "# mps_device=torch.device('mps')\n",
    "\n",
    "# Determine the best device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  torch_dtype=torch.float16, # todo: use 4bit quant\n",
    "  # device_map='auto',\n",
    "  device_map={\"\": device},\n",
    "  # trust_remote_code=True\n",
    "  low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model=model.to(device)\n",
    "\n",
    "prompt='give me a short instruction to large language model'\n",
    "messages=[\n",
    "  {'role': 'system', 'content': 'You are Qwen, a helpful AI assistant'},\n",
    "  {'role': 'user', 'content': prompt}\n",
    "]\n",
    "\n",
    "text=tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  tokenize=False,\n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(text)\n",
    "\n",
    "model_inputs=tokenizer([text], return_tensors='pt').to(model.device)\n",
    "\n",
    "print(model_inputs)\n",
    "\n",
    "# Generate response\n",
    "generated_ids = model.generate(\n",
    "    **model_inputs,\n",
    "    max_new_tokens=512,\n",
    "    do_sample=True,  # Enable sampling for more diverse output\n",
    "    temperature=0.7,  # Moderate randomness\n",
    "    top_p=0.9        # Nucleus sampling\n",
    ")\n",
    "\n",
    "# Decode only the newly generated tokens\n",
    "generated_ids = [\n",
    "    output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "]\n",
    "\n",
    "# Decode and return response\n",
    "response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
