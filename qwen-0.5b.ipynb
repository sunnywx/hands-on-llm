{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MPS limited on macOS < 14.0. Falling back to CPU.\n",
      "Using device: cpu\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Device set to use cpu\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<|im_start|>system\n",
      "You are Qwen, a helpful AI assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "give me a short instruction to large language model<|im_end|>\n",
      "<|im_start|>assistant\n",
      " \n",
      "\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,    264,\n",
      "          10950,  15235,  17847, 151645,    198, 151644,    872,    198,  46430,\n",
      "            752,    264,   2805,   7600,    311,   3460,   4128,   1614, 151645,\n",
      "            198, 151644,  77091,    198]]), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]])} \n",
      "\n",
      "Sure! Here's a classic chicken joke for you:\n",
      "\n",
      "Why did the chicken cross the road?\n",
      "\n",
      "To get to the other side!\n",
      "\n",
      "I hope that brings a smile to your face!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# print(f\"NumPy version: {np.__version__}\")\n",
    "# print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "# Set proxy for requests library\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:1087'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:1087'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pip install ipywidgets\n",
    "\n",
    "then restart jupyter, will show progress bar\n",
    "\"\"\"\n",
    "\n",
    "model_name='Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "def get_device():\n",
    "    if torch.backends.mps.is_available():\n",
    "        # Check macOS version compatibility\n",
    "        import platform\n",
    "        mac_version = platform.mac_ver()[0].split('.')\n",
    "        \n",
    "        # If macOS version is less than 14.0, fall back to CPU\n",
    "        if int(mac_version[0]) < 14:\n",
    "            print(\"MPS limited on macOS < 14.0. Falling back to CPU.\")\n",
    "            return torch.device(\"cpu\")\n",
    "        \n",
    "        return torch.device(\"mps\")\n",
    "    else:\n",
    "        return torch.device(\"cpu\")\n",
    "\n",
    "# Explicitly set default device\n",
    "# torch.set_default_device('cpu')  # or 'mps' for Mac M1/M2\n",
    "\n",
    "# Determine the best device\n",
    "device=get_device()\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  torch_dtype=torch.float16, # todo: use 4bit quant\n",
    "  # device_map='auto',\n",
    "  device_map={\"\": device},\n",
    "  trust_remote_code=True,\n",
    "  low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "prompt='give me a short instruction to large language model'\n",
    "messages=[\n",
    "  {'role': 'system', 'content': 'You are Qwen, a helpful AI assistant'},\n",
    "  {'role': 'user', 'content': prompt}\n",
    "]\n",
    "\n",
    "text=tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  tokenize=False,\n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(text, '\\n')\n",
    "\n",
    "# prepare model inputs\n",
    "model_inputs=tokenizer([text], return_tensors='pt').to(model.device)\n",
    "print(model_inputs, '\\n')\n",
    "\n",
    "# token by token generation\n",
    "def generate_tokens(model, tokenizer, model_inputs, max_new_tokens=512):\n",
    "    # Start generation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initial generation settings\n",
    "        generation_config = {\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'pad_token_id': tokenizer.pad_token_id,\n",
    "        }\n",
    "        \n",
    "        # Create a stream generator\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs, \n",
    "            **generation_config,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict_in_generate=True,\n",
    "            synced_gpus=False,\n",
    "        )\n",
    "        \n",
    "        # Iterate through tokens\n",
    "        for token in generated_ids.sequences[0][len(model_inputs.input_ids[0]):]:\n",
    "            # Decode the single token\n",
    "            decoded_token = tokenizer.decode(token.unsqueeze(0), skip_special_tokens=True)\n",
    "            \n",
    "            # Print the token (flush to see real-time output)\n",
    "            print(decoded_token, end='', flush=True)\n",
    "        \n",
    "        print()  # New line after generation\n",
    "  \n",
    "# generate all tokens once\n",
    "def gen_completion():\n",
    "  # Generate response\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=512,\n",
    "      do_sample=False,  # Enable sampling for more diverse output\n",
    "      # temperature=0.7,  # Moderate randomness\n",
    "      # top_p=0.9        # Nucleus sampling\n",
    "  )\n",
    "\n",
    "  # Decode only the newly generated tokens\n",
    "  generated_ids = [\n",
    "      output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "  ]\n",
    "\n",
    "  # Decode and return response\n",
    "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "  print('resp: ', response)\n",
    "\n",
    "\n",
    "# generate_tokens(model, tokenizer, model_inputs)\n",
    "\n",
    "def run_pipeline():\n",
    "    generator=pipeline(\n",
    "        'text-generation',\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=500,\n",
    "        do_sample=False\n",
    "    )\n",
    "    messages=[\n",
    "        {'role': 'user', 'content': 'tell me a joke about chickens'}\n",
    "    ]\n",
    "    output=generator(messages)\n",
    "    print(output[0]['generated_text'])\n",
    "\n",
    "run_pipeline()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
