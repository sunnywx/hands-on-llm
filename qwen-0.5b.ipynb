{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: mps\n",
      "<|im_start|>system\n",
      "You are Qwen, a helpful AI assistant<|im_end|>\n",
      "<|im_start|>user\n",
      "give me a short instruction to large language model<|im_end|>\n",
      "<|im_start|>assistant\n",
      " \n",
      "\n",
      "{'input_ids': tensor([[151644,   8948,    198,   2610,    525,   1207,  16948,     11,    264,\n",
      "          10950,  15235,  17847, 151645,    198, 151644,    872,    198,  46430,\n",
      "            752,    264,   2805,   7600,    311,   3460,   4128,   1614, 151645,\n",
      "            198, 151644,  77091,    198]], device='mps:0'), 'attention_mask': tensor([[1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
      "         1, 1, 1, 1, 1, 1, 1]], device='mps:0')} \n",
      "\n",
      "Certainly! Here's a simple instruction for a large language model:\n",
      "\n",
      "\"Write a concise summary of the following text in a single paragraph. The text should cover key points and provide an overview of the topic.\"\n",
      "\n",
      "This can be used to quickly generate summaries or summaries of long texts that need to be condensed into shorter forms.\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoModelForCausalLM, AutoTokenizer, pipeline\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "# print(f\"NumPy version: {np.__version__}\")\n",
    "# print(f\"Torch version: {torch.__version__}\")\n",
    "\n",
    "# Set proxy for requests library\n",
    "os.environ['HTTP_PROXY'] = 'http://127.0.0.1:1087'\n",
    "os.environ['HTTPS_PROXY'] = 'http://127.0.0.1:1087'\n",
    "\n",
    "\"\"\"\n",
    "\n",
    "pip install ipywidgets\n",
    "\n",
    "then restart jupyter, will show progress bar\n",
    "\"\"\"\n",
    "\n",
    "model_name='Qwen/Qwen2.5-0.5B-Instruct'\n",
    "\n",
    "# Explicitly set default device\n",
    "# torch.set_default_device('cpu')  # or 'mps' for Mac M1/M2\n",
    "\n",
    "# Determine the best device\n",
    "if torch.backends.mps.is_available():\n",
    "    device = torch.device(\"mps\")\n",
    "else:\n",
    "    device = torch.device(\"cpu\")\n",
    "\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "model=AutoModelForCausalLM.from_pretrained(\n",
    "  model_name,\n",
    "  torch_dtype=torch.float16, # todo: use 4bit quant\n",
    "  # device_map='auto',\n",
    "  device_map={\"\": device},\n",
    "  # trust_remote_code=True\n",
    "  low_cpu_mem_usage=True\n",
    ")\n",
    "\n",
    "tokenizer=AutoTokenizer.from_pretrained(model_name)\n",
    "\n",
    "# model=model.to(device)\n",
    "\n",
    "prompt='give me a short instruction to large language model'\n",
    "messages=[\n",
    "  {'role': 'system', 'content': 'You are Qwen, a helpful AI assistant'},\n",
    "  {'role': 'user', 'content': prompt}\n",
    "]\n",
    "\n",
    "text=tokenizer.apply_chat_template(\n",
    "  messages,\n",
    "  tokenize=False,\n",
    "  add_generation_prompt=True\n",
    ")\n",
    "\n",
    "print(text, '\\n')\n",
    "\n",
    "# prepare model inputs\n",
    "model_inputs=tokenizer([text], return_tensors='pt').to(model.device)\n",
    "print(model_inputs, '\\n')\n",
    "\n",
    "# token by token generation\n",
    "def generate_tokens(model, tokenizer, model_inputs, max_new_tokens=512):\n",
    "    # Start generation\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        # Initial generation settings\n",
    "        generation_config = {\n",
    "            'max_new_tokens': max_new_tokens,\n",
    "            'do_sample': True,\n",
    "            'temperature': 0.7,\n",
    "            'top_p': 0.9,\n",
    "            'pad_token_id': tokenizer.pad_token_id,\n",
    "        }\n",
    "        \n",
    "        # Create a stream generator\n",
    "        generated_ids = model.generate(\n",
    "            **model_inputs, \n",
    "            **generation_config,\n",
    "            output_attentions=False,\n",
    "            output_hidden_states=False,\n",
    "            return_dict_in_generate=True,\n",
    "            synced_gpus=False,\n",
    "        )\n",
    "        \n",
    "        # Iterate through tokens\n",
    "        for token in generated_ids.sequences[0][len(model_inputs.input_ids[0]):]:\n",
    "            # Decode the single token\n",
    "            decoded_token = tokenizer.decode(token.unsqueeze(0), skip_special_tokens=True)\n",
    "            \n",
    "            # Print the token (flush to see real-time output)\n",
    "            print(decoded_token, end='', flush=True)\n",
    "        \n",
    "        print()  # New line after generation\n",
    "  \n",
    "# generate all tokens once\n",
    "def gen_completion():\n",
    "  # Generate response\n",
    "  generated_ids = model.generate(\n",
    "      **model_inputs,\n",
    "      max_new_tokens=512,\n",
    "      do_sample=False,  # Enable sampling for more diverse output\n",
    "      # temperature=0.7,  # Moderate randomness\n",
    "      # top_p=0.9        # Nucleus sampling\n",
    "  )\n",
    "\n",
    "  # Decode only the newly generated tokens\n",
    "  generated_ids = [\n",
    "      output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
    "  ]\n",
    "\n",
    "  # Decode and return response\n",
    "  response = tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
    "\n",
    "  print('resp: ', response)\n",
    "\n",
    "\n",
    "generate_tokens(model, tokenizer, model_inputs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
